import torch
import torch.nn.functional as F
from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification,
    AutoFeatureExtractor, AutoModelForImageClassification,
    Wav2Vec2FeatureExtractor, HubertForSequenceClassification
)
from PIL import Image
import torchaudio

# --- í…ìŠ¤íŠ¸ ì„ë² ë”© (KcBERT fine-tuned) ---
class TextEmbeddingEncoder:
    def __init__(self, model_path="D:/kcbert-emotion-finetuned"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForSequenceClassification.from_pretrained(model_path)
        self.model.eval()

    def encode(self, text):
        inputs = self.tokenizer(text, return_tensors="pt", truncation=True, padding=True)
        with torch.no_grad():
            outputs = self.model(**inputs, output_hidden_states=True)
        embedding = outputs.hidden_states[-1].mean(dim=1).squeeze(0)  # [768]
        return embedding

# --- ì˜¤ë””ì˜¤ ì„ë² ë”© (HuBERT base â†’ 768) ---
class AudioEmbeddingEncoder:
    def __init__(self, model_name="superb/hubert-base-superb-er"):  # âœ… baseë¡œ ë³€ê²½
        self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)
        self.model = HubertForSequenceClassification.from_pretrained(model_name)
        self.model.eval()

    def encode(self, audio_path):
        waveform, sample_rate = torchaudio.load(audio_path)
        waveform = waveform.mean(dim=0, keepdim=True)
        if sample_rate != 16000:
            waveform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)(waveform)
        inputs = self.feature_extractor(waveform.squeeze().numpy(), sampling_rate=16000, return_tensors="pt")
        with torch.no_grad():
            outputs = self.model(**inputs, output_hidden_states=True)
        embedding = outputs.hidden_states[-1].mean(dim=1).squeeze(0)  # âœ… [768]
        return embedding

# --- ì´ë¯¸ì§€ ì„ë² ë”© (ViT) ---
class ImageEmbeddingEncoder:
    def __init__(self, model_name="trpakov/vit-face-expression"):
        self.extractor = AutoFeatureExtractor.from_pretrained(model_name)
        self.model = AutoModelForImageClassification.from_pretrained(model_name)
        self.model.eval()

    def encode(self, image_path):
        image = Image.open(image_path).convert("RGB")
        inputs = self.extractor(images=image, return_tensors="pt")
        with torch.no_grad():
            outputs = self.model(**inputs, output_hidden_states=True)
        embedding = outputs.hidden_states[-1].mean(dim=1).squeeze(0)  # [768]
        return embedding

# --- Attention Fusion ---
def attention_fusion(vectors, query_idx=0):
    print(f"[Fusion] â–¶ï¸ ì…ë ¥ ë²¡í„° ìˆ˜: {len(vectors)}")
    for i, v in enumerate(vectors):
        print(f"[Fusion] ğŸ”¹ ë²¡í„° {i} í¬ê¸°: {v.shape}")

    stacked = torch.stack(vectors)             # [n, 768]
    print(f"[Fusion] âœ… stacked shape: {stacked.shape}")

    Q = vectors[query_idx].unsqueeze(0)        # [1, 768]
    print(f"[Fusion] âœ… Q shape: {Q.shape}")

    attn_score = torch.matmul(Q, stacked.T) / Q.shape[-1] ** 0.5  # [1, n]
    print(f"[Fusion] âœ… Attention score shape: {attn_score.shape}")

    weights = F.softmax(attn_score, dim=-1)    # [1, n]
    print(f"[Fusion] âœ… Softmax weights: {weights}")

    fused = torch.matmul(weights, stacked).squeeze(0)  # shape: [768]
    print(f"[Fusion] âœ… Fused vector shape: {fused.shape}")

    return fused


def summarize_vector(name, vec):
    normed = F.normalize(vec, dim=0)  # L2 ì •ê·œí™”
    print(f"ğŸ“Œ [{name}] Summary")
    print(f" - Shape       : {normed.shape}")
    print(f" - Min         : {normed.min().item():.4f}")
    print(f" - Max         : {normed.max().item():.4f}")
    print(f" - Mean        : {normed.mean().item():.4f}")
    print(f" - Std Dev     : {normed.std().item():.4f}")
    return normed


# --- ì˜ˆì‹œ ì‹¤í–‰ (ì§ì ‘ ì‹¤í–‰ ì‹œ)
if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì…ë ¥
    text_input = "ì•„ ì™œì´ë ‡ê²Œ ì¢†ê°™ëƒ ì‹œë°œ ì§„ì í…ìŠ¤íŠ¸ëŠ” ì œëŒ€ë¡œ ë¶„ì„ë„ ëª»í•˜ë‚˜ ë³‘ì‹ ì´ ê°’ì´ ì™œ ì œëŒ€ë¡œ ì•ˆë‚˜ì˜¤ëŠ”ë°?"
    audio_path = "example.wav"
    image_path = "example.jpg"

    # ì¸ì½”ë” ë¡œë”©
    text_encoder = TextEmbeddingEncoder()
    audio_encoder = AudioEmbeddingEncoder()
    image_encoder = ImageEmbeddingEncoder()

    # ê° ì„ë² ë”© ë²¡í„° ì¶”ì¶œ ë° L2 ì •ê·œí™”
    text_vec = summarize_vector("Text", text_encoder.encode(text_input))
    audio_vec = summarize_vector("Audio", audio_encoder.encode(audio_path))
    image_vec = summarize_vector("Image", image_encoder.encode(image_path))

    # Attention Fusion (ê¸°ì¤€: ì´ë¯¸ì§€)
    fused_vec = attention_fusion([text_vec, audio_vec, image_vec], query_idx=2) 

    # ì¶œë ¥
    print("ğŸ¯ Fused Vector Shape:", fused_vec.shape)
    print("ğŸ’¡ First 10 Values:", fused_vec[:10].tolist())
