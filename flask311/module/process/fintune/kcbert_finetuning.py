import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from datasets import Dataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import numpy as np

print("âœ… CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€:", torch.cuda.is_available())

# 1. ì—‘ì…€ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
df = pd.read_excel("test.xlsx")
df = df[['Sentence', 'Emotion']].copy()
df.columns = ['sentence', 'label']

# 2. ê°ì • ë¼ë²¨ ìˆ«ì ì¸ë±ìŠ¤ë¡œ ë§¤í•‘
label2id = {'ê³µí¬': 0, 'ë†€ëŒ': 1, 'ë¶„ë…¸': 2, 'ìŠ¬í””': 3, 'ì¤‘ë¦½': 4, 'í–‰ë³µ': 5, 'í˜ì˜¤': 6}
df = df[df['label'].isin(label2id.keys())].copy()
df['label'] = df['label'].map(label2id)

# 3. HuggingFace Datasetìœ¼ë¡œ ë³€í™˜
dataset = Dataset.from_pandas(df)

# 4. Tokenizer ë¡œë”© ë° í† í¬ë‚˜ì´ì§•
model_name = "beomi/KcBERT-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)

def tokenize(example):
    return tokenizer(example['sentence'], truncation=True, padding='max_length', max_length=128)

dataset = dataset.map(tokenize)

# 5. í•™ìŠµ/ê²€ì¦ ë¶„í• 
dataset = dataset.train_test_split(test_size=0.1)
train_dataset = dataset['train']
eval_dataset = dataset['test']

# 6. ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° (num_labels=7)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=7)

# 7. í•™ìŠµ ì¸ì ì„¤ì • (Dë“œë¼ì´ë¸Œì— ì €ì¥)
training_args = TrainingArguments(
    output_dir="D:/kcbert-emotion-checkpoints",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    save_total_limit=2,
    load_best_model_at_end=True,
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir="D:/kcbert-emotion-logs",
    logging_steps=100
)

# 8. Trainer êµ¬ì„±
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
)

# 9. í•™ìŠµ ì‹œì‘
trainer.train()

# 10. ìµœì¢… ëª¨ë¸ ì €ì¥ (Dë“œë¼ì´ë¸Œ)
trainer.save_model("D:/kcbert-emotion-finetuned")
tokenizer.save_pretrained("D:/kcbert-emotion-finetuned")

# ì˜ˆì¸¡
predictions = trainer.predict(eval_dataset)
preds = np.argmax(predictions.predictions, axis=1)
labels = predictions.label_ids

# ë¼ë²¨ ë³µì›
id2label = {v: k for k, v in label2id.items()}
target_names = [id2label[i] for i in range(len(id2label))]

# ì„±ëŠ¥ ë¶„ì„í‘œ ì¶œë ¥
report = classification_report(labels, preds, target_names=target_names, digits=4)
print("ğŸ“Š ê°ì •ë³„ ì„±ëŠ¥ ë¶„ì„:\n")
print(report)