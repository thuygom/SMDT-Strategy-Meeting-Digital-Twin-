{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d35916a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "print(\"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€:\", torch.cuda.is_available())\n",
    "\n",
    "# 1. ì—‘ì…€ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df = pd.read_excel(\"test.xlsx\")\n",
    "df = df[['Sentence', 'Emotion']].copy()\n",
    "df.columns = ['sentence', 'label']\n",
    "\n",
    "# 2. ê°ì • ë¼ë²¨ ìˆ«ì ì¸ë±ìŠ¤ë¡œ ë§¤í•‘\n",
    "label2id = {'ê³µí¬': 0, 'ë†€ëŒ': 1, 'ë¶„ë…¸': 2, 'ìŠ¬í””': 3, 'ì¤‘ë¦½': 4, 'í–‰ë³µ': 5, 'í˜ì˜¤': 6}\n",
    "df = df[df['label'].isin(label2id.keys())].copy()\n",
    "df['label'] = df['label'].map(label2id)\n",
    "\n",
    "# 3. HuggingFace Datasetìœ¼ë¡œ ë³€í™˜\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# 4. Tokenizer ë¡œë”© ë° í† í¬ë‚˜ì´ì§•\n",
    "model_name = \"beomi/KcBERT-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize(example):\n",
    "    return tokenizer(example['sentence'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "dataset = dataset.map(tokenize)\n",
    "\n",
    "# 5. í•™ìŠµ/ê²€ì¦ ë¶„í• \n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = dataset['train']\n",
    "eval_dataset = dataset['test']\n",
    "\n",
    "# 6. ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° (num_labels=7)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=7)\n",
    "\n",
    "# 7. í•™ìŠµ ì¸ì ì„¤ì • (Dë“œë¼ì´ë¸Œì— ì €ì¥)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"D:/kcbert-emotion-checkpoints\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"D:/kcbert-emotion-logs\",\n",
    "    logging_steps=100\n",
    ")\n",
    "\n",
    "# 8. Trainer êµ¬ì„±\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "# 9. í•™ìŠµ ì‹œì‘\n",
    "trainer.train()\n",
    "\n",
    "# 10. ìµœì¢… ëª¨ë¸ ì €ì¥ (Dë“œë¼ì´ë¸Œ)\n",
    "trainer.save_model(\"D:/kcbert-emotion-finetuned\")\n",
    "tokenizer.save_pretrained(\"D:/kcbert-emotion-finetuned\")\n",
    "\n",
    "# ì˜ˆì¸¡\n",
    "predictions = trainer.predict(eval_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=1)\n",
    "labels = predictions.label_ids\n",
    "\n",
    "# ë¼ë²¨ ë³µì›\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "target_names = [id2label[i] for i in range(len(id2label))]\n",
    "\n",
    "# ì„±ëŠ¥ ë¶„ì„í‘œ ì¶œë ¥\n",
    "report = classification_report(labels, preds, target_names=target_names, digits=4)\n",
    "print(\"ğŸ“Š ê°ì •ë³„ ì„±ëŠ¥ ë¶„ì„:\\n\")\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
