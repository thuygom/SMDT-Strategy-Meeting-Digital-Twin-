{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753829b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import pandas as pd\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "def get_sample_rate(file_path):\n",
    "    \"\"\"WAV 파일의 샘플 레이트를 확인합니다.\"\"\"\n",
    "    with wave.open(file_path, 'rb') as wf:\n",
    "        sample_rate = wf.getframerate()\n",
    "    return sample_rate\n",
    "\n",
    "def convert_to_mono(audio):\n",
    "    \"\"\"오디오 파일을 모노로 변환합니다.\"\"\"\n",
    "    if audio.channels != 1:\n",
    "        audio = audio.set_channels(1)\n",
    "    return audio\n",
    "\n",
    "def convert_to_16bit(audio):\n",
    "    \"\"\"WAV 파일을 16비트 샘플로 변환합니다.\"\"\"\n",
    "    return audio.set_sample_width(2)  # 16비트 샘플\n",
    "\n",
    "def split_audio_by_speaker(file_path, diarization_results, output_dir):\n",
    "    \"\"\"\n",
    "    주어진 음성 파일을 화자별로 나누어 저장하는 함수. \n",
    "    동일 화자의 음성은 하나의 파일로 묶어 저장합니다.\n",
    "    \n",
    "    Args:\n",
    "    - file_path (str): 원본 음성 파일 경로\n",
    "    - diarization_results (list): 다이어리제이션 결과 (화자, 시작, 종료 시간)\n",
    "    - output_dir (str): 화자별로 음성 파일을 저장할 디렉토리\n",
    "    \"\"\"\n",
    "    # 오디오 파일 로드 및 변환\n",
    "    audio = AudioSegment.from_file(file_path)\n",
    "    audio = convert_to_mono(audio)\n",
    "    audio = convert_to_16bit(audio)\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # 화자별로 발언들을 모을 딕셔너리\n",
    "    speaker_audio_chunks = {}\n",
    "\n",
    "    # 화자별로 음성을 모아서 하나의 오디오로 합침\n",
    "    for segment in diarization_results:\n",
    "        start_time = segment['start'] * 1000  # milliseconds\n",
    "        stop_time = segment['stop'] * 1000  # milliseconds\n",
    "        speaker = segment['speaker']\n",
    "        \n",
    "        # 해당 구간의 오디오 조각 추출\n",
    "        audio_chunk = audio[start_time:stop_time]\n",
    "\n",
    "        # 같은 화자의 음성을 모은 후 합치기\n",
    "        if speaker not in speaker_audio_chunks:\n",
    "            speaker_audio_chunks[speaker] = audio_chunk\n",
    "        else:\n",
    "            speaker_audio_chunks[speaker] += audio_chunk\n",
    "\n",
    "    # 화자별로 음성 파일을 저장\n",
    "    for speaker, combined_audio in speaker_audio_chunks.items():\n",
    "        speaker_file = os.path.join(output_dir, f\"speaker_{speaker}.wav\")\n",
    "        combined_audio.export(speaker_file, format=\"wav\")\n",
    "        print(f\"화자 {speaker}의 합쳐진 음성 파일을 저장: {speaker_file}\")\n",
    "\n",
    "def load_diarization_results(excel_path):\n",
    "    \"\"\"엑셀 파일에서 다이어리제이션 결과를 읽어옵니다.\"\"\"\n",
    "    # 엑셀 파일에서 다이어리제이션 결과 읽기\n",
    "    df = pd.read_excel(excel_path)\n",
    "    \n",
    "    # 다이어리제이션 결과를 딕셔너리 형태로 변환\n",
    "    diarization_results = []\n",
    "    for _, row in df.iterrows():\n",
    "        diarization_results.append({\n",
    "            'start': row['start'],\n",
    "            'stop': row['stop'],\n",
    "            'speaker': row['speaker']\n",
    "        })\n",
    "    return diarization_results\n",
    "\n",
    "def main():\n",
    "    # 커맨드라인 인자 처리\n",
    "    parser = argparse.ArgumentParser(description=\"음성 파일을 화자별로 나누어 저장합니다.\")\n",
    "    parser.add_argument('audio_file', type=str, help=\"WAV 오디오 파일 경로\")\n",
    "    parser.add_argument('diarization_excel', type=str, help=\"화자 분리 결과가 저장된 엑셀 파일 경로\")\n",
    "    parser.add_argument('output_dir', type=str, help=\"화자별 음성 파일을 저장할 디렉토리\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # 다이어리제이션 결과를 엑셀에서 불러오기\n",
    "    diarization_results = load_diarization_results(args.diarization_excel)\n",
    "    \n",
    "    # 화자별 음성 분리\n",
    "    split_audio_by_speaker(args.audio_file, diarization_results, args.output_dir)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
