{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aae44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    AutoFeatureExtractor, AutoModelForImageClassification,\n",
    "    Wav2Vec2FeatureExtractor, HubertForSequenceClassification\n",
    ")\n",
    "from PIL import Image\n",
    "import torchaudio\n",
    "\n",
    "# --- í…ìŠ¤íŠ¸ ì„ë² ë”© (KcBERT fine-tuned) ---\n",
    "class TextEmbeddingEncoder:\n",
    "    def __init__(self, model_path=\"D:/kcbert-emotion-finetuned\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "        self.model.eval()\n",
    "\n",
    "    def encode(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs, output_hidden_states=True)\n",
    "        embedding = outputs.hidden_states[-1].mean(dim=1).squeeze(0)  # [768]\n",
    "        return embedding\n",
    "\n",
    "# --- ì˜¤ë””ì˜¤ ì„ë² ë”© (HuBERT base â†’ 768) ---\n",
    "class AudioEmbeddingEncoder:\n",
    "    def __init__(self, model_name=\"superb/hubert-base-superb-er\"):  # âœ… baseë¡œ ë³€ê²½\n",
    "        self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "        self.model = HubertForSequenceClassification.from_pretrained(model_name)\n",
    "        self.model.eval()\n",
    "\n",
    "    def encode(self, audio_path):\n",
    "        waveform, sample_rate = torchaudio.load(audio_path)\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "        if sample_rate != 16000:\n",
    "            waveform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)(waveform)\n",
    "        inputs = self.feature_extractor(waveform.squeeze().numpy(), sampling_rate=16000, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs, output_hidden_states=True)\n",
    "        embedding = outputs.hidden_states[-1].mean(dim=1).squeeze(0)  # âœ… [768]\n",
    "        return embedding\n",
    "\n",
    "# --- ì´ë¯¸ì§€ ì„ë² ë”© (ViT) ---\n",
    "class ImageEmbeddingEncoder:\n",
    "    def __init__(self, model_name=\"trpakov/vit-face-expression\"):\n",
    "        self.extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
    "        self.model = AutoModelForImageClassification.from_pretrained(model_name)\n",
    "        self.model.eval()\n",
    "\n",
    "    def encode(self, image_path):\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        inputs = self.extractor(images=image, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs, output_hidden_states=True)\n",
    "        embedding = outputs.hidden_states[-1].mean(dim=1).squeeze(0)  # [768]\n",
    "        return embedding\n",
    "\n",
    "# --- Attention Fusion ---\n",
    "def attention_fusion(vectors, query_idx=0):\n",
    "    print(f\"[Fusion] â–¶ï¸ ì…ë ¥ ë²¡í„° ìˆ˜: {len(vectors)}\")\n",
    "    for i, v in enumerate(vectors):\n",
    "        print(f\"[Fusion] ğŸ”¹ ë²¡í„° {i} í¬ê¸°: {v.shape}\")\n",
    "\n",
    "    stacked = torch.stack(vectors)             # [n, 768]\n",
    "    print(f\"[Fusion] âœ… stacked shape: {stacked.shape}\")\n",
    "\n",
    "    Q = vectors[query_idx].unsqueeze(0)        # [1, 768]\n",
    "    print(f\"[Fusion] âœ… Q shape: {Q.shape}\")\n",
    "\n",
    "    attn_score = torch.matmul(Q, stacked.T) / Q.shape[-1] ** 0.5  # [1, n]\n",
    "    print(f\"[Fusion] âœ… Attention score shape: {attn_score.shape}\")\n",
    "\n",
    "    weights = F.softmax(attn_score, dim=-1)    # [1, n]\n",
    "    print(f\"[Fusion] âœ… Softmax weights: {weights}\")\n",
    "\n",
    "    fused = torch.matmul(weights, stacked).squeeze(0)  # shape: [768]\n",
    "    print(f\"[Fusion] âœ… Fused vector shape: {fused.shape}\")\n",
    "\n",
    "    return fused\n",
    "\n",
    "\n",
    "def summarize_vector(name, vec):\n",
    "    normed = F.normalize(vec, dim=0)  # L2 ì •ê·œí™”\n",
    "    print(f\"ğŸ“Œ [{name}] Summary\")\n",
    "    print(f\" - Shape       : {normed.shape}\")\n",
    "    print(f\" - Min         : {normed.min().item():.4f}\")\n",
    "    print(f\" - Max         : {normed.max().item():.4f}\")\n",
    "    print(f\" - Mean        : {normed.mean().item():.4f}\")\n",
    "    print(f\" - Std Dev     : {normed.std().item():.4f}\")\n",
    "    return normed\n",
    "\n",
    "\n",
    "# --- ì˜ˆì‹œ ì‹¤í–‰ (ì§ì ‘ ì‹¤í–‰ ì‹œ)\n",
    "if __name__ == \"__main__\":\n",
    "    # í…ŒìŠ¤íŠ¸ ì…ë ¥\n",
    "    text_input = \"ì•„ ì™œì´ë ‡ê²Œ ì¢†ê°™ëƒ ì‹œë°œ ì§„ì í…ìŠ¤íŠ¸ëŠ” ì œëŒ€ë¡œ ë¶„ì„ë„ ëª»í•˜ë‚˜ ë³‘ì‹ ì´ ê°’ì´ ì™œ ì œëŒ€ë¡œ ì•ˆë‚˜ì˜¤ëŠ”ë°?\"\n",
    "    audio_path = \"example.wav\"\n",
    "    image_path = \"example.jpg\"\n",
    "\n",
    "    # ì¸ì½”ë” ë¡œë”©\n",
    "    text_encoder = TextEmbeddingEncoder()\n",
    "    audio_encoder = AudioEmbeddingEncoder()\n",
    "    image_encoder = ImageEmbeddingEncoder()\n",
    "\n",
    "    # ê° ì„ë² ë”© ë²¡í„° ì¶”ì¶œ ë° L2 ì •ê·œí™”\n",
    "    text_vec = summarize_vector(\"Text\", text_encoder.encode(text_input))\n",
    "    audio_vec = summarize_vector(\"Audio\", audio_encoder.encode(audio_path))\n",
    "    image_vec = summarize_vector(\"Image\", image_encoder.encode(image_path))\n",
    "\n",
    "    # Attention Fusion (ê¸°ì¤€: ì´ë¯¸ì§€)\n",
    "    fused_vec = attention_fusion([text_vec, audio_vec, image_vec], query_idx=2) \n",
    "\n",
    "    # ì¶œë ¥\n",
    "    print(\"ğŸ¯ Fused Vector Shape:\", fused_vec.shape)\n",
    "    print(\"ğŸ’¡ First 10 Values:\", fused_vec[:10].tolist())\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
