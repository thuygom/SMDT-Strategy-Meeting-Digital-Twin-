{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aae44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    AutoFeatureExtractor, AutoModelForImageClassification,\n",
    "    Wav2Vec2FeatureExtractor, HubertForSequenceClassification\n",
    ")\n",
    "from PIL import Image\n",
    "import torchaudio\n",
    "\n",
    "# --- 텍스트 임베딩 (KcBERT fine-tuned) ---\n",
    "class TextEmbeddingEncoder:\n",
    "    def __init__(self, model_path=\"D:/kcbert-emotion-finetuned\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "        self.model.eval()\n",
    "\n",
    "    def encode(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs, output_hidden_states=True)\n",
    "        embedding = outputs.hidden_states[-1].mean(dim=1).squeeze(0)  # [768]\n",
    "        return embedding\n",
    "\n",
    "# --- 오디오 임베딩 (HuBERT base → 768) ---\n",
    "class AudioEmbeddingEncoder:\n",
    "    def __init__(self, model_name=\"superb/hubert-base-superb-er\"):  # ✅ base로 변경\n",
    "        self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "        self.model = HubertForSequenceClassification.from_pretrained(model_name)\n",
    "        self.model.eval()\n",
    "\n",
    "    def encode(self, audio_path):\n",
    "        waveform, sample_rate = torchaudio.load(audio_path)\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "        if sample_rate != 16000:\n",
    "            waveform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)(waveform)\n",
    "        inputs = self.feature_extractor(waveform.squeeze().numpy(), sampling_rate=16000, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs, output_hidden_states=True)\n",
    "        embedding = outputs.hidden_states[-1].mean(dim=1).squeeze(0)  # ✅ [768]\n",
    "        return embedding\n",
    "\n",
    "# --- 이미지 임베딩 (ViT) ---\n",
    "class ImageEmbeddingEncoder:\n",
    "    def __init__(self, model_name=\"trpakov/vit-face-expression\"):\n",
    "        self.extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
    "        self.model = AutoModelForImageClassification.from_pretrained(model_name)\n",
    "        self.model.eval()\n",
    "\n",
    "    def encode(self, image_path):\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        inputs = self.extractor(images=image, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs, output_hidden_states=True)\n",
    "        embedding = outputs.hidden_states[-1].mean(dim=1).squeeze(0)  # [768]\n",
    "        return embedding\n",
    "\n",
    "# --- Attention Fusion ---\n",
    "def attention_fusion(vectors, query_idx=0):\n",
    "    print(f\"[Fusion] ▶️ 입력 벡터 수: {len(vectors)}\")\n",
    "    for i, v in enumerate(vectors):\n",
    "        print(f\"[Fusion] 🔹 벡터 {i} 크기: {v.shape}\")\n",
    "\n",
    "    stacked = torch.stack(vectors)             # [n, 768]\n",
    "    print(f\"[Fusion] ✅ stacked shape: {stacked.shape}\")\n",
    "\n",
    "    Q = vectors[query_idx].unsqueeze(0)        # [1, 768]\n",
    "    print(f\"[Fusion] ✅ Q shape: {Q.shape}\")\n",
    "\n",
    "    attn_score = torch.matmul(Q, stacked.T) / Q.shape[-1] ** 0.5  # [1, n]\n",
    "    print(f\"[Fusion] ✅ Attention score shape: {attn_score.shape}\")\n",
    "\n",
    "    weights = F.softmax(attn_score, dim=-1)    # [1, n]\n",
    "    print(f\"[Fusion] ✅ Softmax weights: {weights}\")\n",
    "\n",
    "    fused = torch.matmul(weights, stacked).squeeze(0)  # shape: [768]\n",
    "    print(f\"[Fusion] ✅ Fused vector shape: {fused.shape}\")\n",
    "\n",
    "    return fused\n",
    "\n",
    "\n",
    "def summarize_vector(name, vec):\n",
    "    normed = F.normalize(vec, dim=0)  # L2 정규화\n",
    "    print(f\"📌 [{name}] Summary\")\n",
    "    print(f\" - Shape       : {normed.shape}\")\n",
    "    print(f\" - Min         : {normed.min().item():.4f}\")\n",
    "    print(f\" - Max         : {normed.max().item():.4f}\")\n",
    "    print(f\" - Mean        : {normed.mean().item():.4f}\")\n",
    "    print(f\" - Std Dev     : {normed.std().item():.4f}\")\n",
    "    return normed\n",
    "\n",
    "\n",
    "# --- 예시 실행 (직접 실행 시)\n",
    "if __name__ == \"__main__\":\n",
    "    # 테스트 입력\n",
    "    text_input = \"아 왜이렇게 좆같냐 시발 진자 텍스트는 제대로 분석도 못하나 병신이 값이 왜 제대로 안나오는데?\"\n",
    "    audio_path = \"example.wav\"\n",
    "    image_path = \"example.jpg\"\n",
    "\n",
    "    # 인코더 로딩\n",
    "    text_encoder = TextEmbeddingEncoder()\n",
    "    audio_encoder = AudioEmbeddingEncoder()\n",
    "    image_encoder = ImageEmbeddingEncoder()\n",
    "\n",
    "    # 각 임베딩 벡터 추출 및 L2 정규화\n",
    "    text_vec = summarize_vector(\"Text\", text_encoder.encode(text_input))\n",
    "    audio_vec = summarize_vector(\"Audio\", audio_encoder.encode(audio_path))\n",
    "    image_vec = summarize_vector(\"Image\", image_encoder.encode(image_path))\n",
    "\n",
    "    # Attention Fusion (기준: 이미지)\n",
    "    fused_vec = attention_fusion([text_vec, audio_vec, image_vec], query_idx=2) \n",
    "\n",
    "    # 출력\n",
    "    print(\"🎯 Fused Vector Shape:\", fused_vec.shape)\n",
    "    print(\"💡 First 10 Values:\", fused_vec[:10].tolist())\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
