{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5c1833",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import Wav2Vec2FeatureExtractor, HubertForSequenceClassification\n",
    "import torchaudio\n",
    "import torch\n",
    "\n",
    "class HuBERTEmotionClassifier:\n",
    "    def __init__(self, model_name=\"superb/hubert-large-superb-er\"):\n",
    "        self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "        self.model = HubertForSequenceClassification.from_pretrained(model_name)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.label_map = {\n",
    "            0: \"neutral\",\n",
    "            1: \"calm\",\n",
    "            2: \"happy\",\n",
    "            3: \"sad\",\n",
    "            4: \"angry\",\n",
    "            5: \"fearful\",\n",
    "            6: \"disgust\",\n",
    "            7: \"surprised\"\n",
    "        }\n",
    "\n",
    "    def predict_emotion(self, audio_path):\n",
    "        waveform, sample_rate = torchaudio.load(audio_path)\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)  # Convert stereo to mono if needed\n",
    "        if sample_rate != 16000:\n",
    "            waveform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)(waveform)\n",
    "\n",
    "        inputs = self.feature_extractor(waveform.squeeze().numpy(), sampling_rate=16000, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        pred = torch.argmax(outputs.logits, dim=1).item()\n",
    "        return self.label_map[pred]\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    classifier = HuBERTEmotionClassifier()\n",
    "    emotion = classifier.predict_emotion(\"example.wav\")\n",
    "    print(\"예측된 감정:\", emotion)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
