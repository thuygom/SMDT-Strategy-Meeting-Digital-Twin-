{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cceb1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import argparse\n",
    "import os\n",
    "from fvcore.nn import FlopCountAnalysis\n",
    "\n",
    "from cmu_dataset import CMUMOSEIDataset\n",
    "from cmu_cmt import CrossModalTransformer  # âœ… ë§ˆìŠ¤í¬ ì—†ëŠ” full pairwise CMT\n",
    "\n",
    "\n",
    "# âœ… Forward Wrapper (fvcoreìš©)\n",
    "class ForwardOnlyWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, text, audio, visual):\n",
    "        logits, _ = self.model(text, audio, visual)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# âœ… FLOPs ì¸¡ì •\n",
    "def estimate_flops(model, device):\n",
    "    model.eval()\n",
    "    dummy_text = torch.randn(1, 50, 300).to(device)\n",
    "    dummy_audio = torch.randn(1, 50, 74).to(device)\n",
    "    dummy_visual = torch.randn(1, 50, 35).to(device)\n",
    "\n",
    "    wrapped_model = ForwardOnlyWrapper(model)\n",
    "    flops = FlopCountAnalysis(wrapped_model, (dummy_text, dummy_audio, dummy_visual))\n",
    "\n",
    "    print(f\"ðŸ”§ Estimated FLOPs (1 sample @ seq_len=50): {flops.total() / 1e6:.2f} MFLOPs\")\n",
    "\n",
    "\n",
    "# âœ… Collate function\n",
    "def cmu_collate_fn(batch):\n",
    "    texts, audios, visuals, labels7, labels2 = zip(*batch)\n",
    "    padded_texts = pad_sequence(texts, batch_first=True)\n",
    "    padded_audios = pad_sequence(audios, batch_first=True)\n",
    "    padded_visuals = pad_sequence(visuals, batch_first=True)\n",
    "    labels7 = torch.tensor(labels7)\n",
    "    labels2 = torch.tensor(labels2)\n",
    "    return padded_texts, padded_audios, padded_visuals, labels7, labels2\n",
    "\n",
    "\n",
    "# âœ… Evaluation\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct7, correct2, total = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for text, audio, visual, labels7, labels2 in loader:\n",
    "            text, audio, visual = text.to(device), audio.to(device), visual.to(device)\n",
    "            labels7, labels2 = labels7.to(device), labels2.to(device)\n",
    "\n",
    "            logits, _ = model(text, audio, visual)\n",
    "            preds7 = logits.argmax(dim=1)\n",
    "            preds2 = (preds7 >= 3).long()\n",
    "\n",
    "            correct7 += (preds7 == labels7).sum().item()\n",
    "            correct2 += (preds2 == labels2).sum().item()\n",
    "            total += labels7.size(0)\n",
    "\n",
    "    return correct7 / total, correct2 / total\n",
    "\n",
    "\n",
    "# âœ… Main\n",
    "def main(args):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"âœ… Using device: {device}\")\n",
    "\n",
    "    # Load dataset\n",
    "    test_dataset = CMUMOSEIDataset(split='test', data_dir=args.data_dir)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, collate_fn=cmu_collate_fn)\n",
    "\n",
    "    # Load full CMT model (ëª¨ë“  ê²½ë¡œ í™œì„±í™”)\n",
    "    model = CrossModalTransformer(\n",
    "        dim_text=300,\n",
    "        dim_audio=74,\n",
    "        dim_visual=35,\n",
    "        dim_model=args.dim_model,\n",
    "        n_heads=args.n_heads,\n",
    "        dropout=args.dropout\n",
    "    ).to(device)\n",
    "\n",
    "    model.load_state_dict(torch.load(args.model_path, map_location=device))\n",
    "\n",
    "    print(\"\\nðŸ“Š Test Accuracy:\")\n",
    "    acc7, acc2 = evaluate(model, test_loader, device)\n",
    "    print(f\"âœ… 7-class Accuracy: {acc7:.4f}\")\n",
    "    print(f\"âœ… 2-class Accuracy: {acc2:.4f}\")\n",
    "\n",
    "    # FLOPs ê³„ì‚°\n",
    "    estimate_flops(model, device)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--data_dir\", type=str, default=\"../cmu_dataset\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=64)\n",
    "    parser.add_argument(\"--dim_model\", type=int, default=256)\n",
    "    parser.add_argument(\"--n_heads\", type=int, default=8)\n",
    "    parser.add_argument(\"--dropout\", type=float, default=0.3)\n",
    "    parser.add_argument(\"--model_path\", type=str, default=\"./best_model_cmu.pth\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    main(args)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
