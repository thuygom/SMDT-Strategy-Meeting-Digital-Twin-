{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7c3ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SharedCrossModalTransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, n_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=dim, num_heads=n_heads, dropout=dropout, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim * 4, dim)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, q, kv):\n",
    "        attn_output, _ = self.attn(q, kv, kv)\n",
    "        x = self.norm1(q + attn_output)\n",
    "        ffn_output = self.ffn(x)\n",
    "        return self.norm2(x + ffn_output)\n",
    "\n",
    "\n",
    "class CrossModalSuperNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Cross-Modal SuperNet for CMU-MOSEI.\n",
    "    Shared Transformer block + attention path mask + FLOPs estimation.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_text=300, dim_audio=74, dim_visual=35, dim_model=128, n_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dim_model = dim_model\n",
    "\n",
    "        # Projection layers: input modality → shared dim\n",
    "        self.text_proj = nn.Sequential(\n",
    "            nn.Linear(dim_text, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, dim_model)\n",
    "        )\n",
    "        self.audio_proj = nn.Sequential(\n",
    "            nn.Linear(dim_audio, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, dim_model)\n",
    "        )\n",
    "        self.visual_proj = nn.Sequential(\n",
    "            nn.Linear(dim_visual, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, dim_model)\n",
    "        )\n",
    "\n",
    "        # Shared Transformer block\n",
    "        self.shared_block = SharedCrossModalTransformerBlock(dim_model, n_heads, dropout)\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(dim_model, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 7)  # CMU-MOSEI 7-class\n",
    "        )\n",
    "\n",
    "        # FLOPs per modal path (custom static estimate)\n",
    "        self.path_flops = self.get_default_path_flops()\n",
    "\n",
    "    def forward(self, text, audio, visual, mask=None, return_flops=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            text/audio/visual: [B, T, F]\n",
    "            mask: list of 6 binary flags (path 활성화)\n",
    "            return_flops: True일 경우 총 연산량 반환\n",
    "\n",
    "        Returns:\n",
    "            logits: [B, 7]\n",
    "            fused: [B, dim_model]\n",
    "            flops (optional): float\n",
    "        \"\"\"\n",
    "        if mask is None:\n",
    "            mask = [1] * 6  # 모든 path 사용\n",
    "\n",
    "        # Project to shared space\n",
    "        text = self.text_proj(text)\n",
    "        audio = self.audio_proj(audio)\n",
    "        visual = self.visual_proj(visual)\n",
    "\n",
    "        fused_outputs = []\n",
    "\n",
    "        if mask[0]:  # text → audio\n",
    "            fused_outputs.append(self.shared_block(text, audio).mean(1))\n",
    "        if mask[1]:  # text → visual\n",
    "            fused_outputs.append(self.shared_block(text, visual).mean(1))\n",
    "        if mask[2]:  # audio → text\n",
    "            fused_outputs.append(self.shared_block(audio, text).mean(1))\n",
    "        if mask[3]:  # audio → visual\n",
    "            fused_outputs.append(self.shared_block(audio, visual).mean(1))\n",
    "        if mask[4]:  # visual → text\n",
    "            fused_outputs.append(self.shared_block(visual, text).mean(1))\n",
    "        if mask[5]:  # visual → audio\n",
    "            fused_outputs.append(self.shared_block(visual, audio).mean(1))\n",
    "\n",
    "        if len(fused_outputs) == 0:\n",
    "            raise ValueError(\"At least one cross-modal path must be active.\")\n",
    "\n",
    "        fused = torch.mean(torch.stack(fused_outputs, dim=0), dim=0)\n",
    "        logits = self.classifier(fused)\n",
    "\n",
    "        if return_flops:\n",
    "            return logits, fused, self.compute_flops(mask)\n",
    "        else:\n",
    "            return logits, fused\n",
    "\n",
    "    def compute_flops(self, mask):\n",
    "        \"\"\"\n",
    "        Sum estimated FLOPs for activated paths.\n",
    "        \"\"\"\n",
    "        if len(mask) != 6:\n",
    "            raise ValueError(\"mask must be length 6\")\n",
    "        return sum(self.path_flops[i] for i in range(6) if mask[i])\n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_path_flops():\n",
    "        \"\"\"\n",
    "        Approximate FLOPs per path for CMU-MOSEI config.\n",
    "        Order:\n",
    "          0: text→audio\n",
    "          1: text→visual\n",
    "          2: audio→text\n",
    "          3: audio→visual\n",
    "          4: visual→text\n",
    "          5: visual→audio\n",
    "        \"\"\"\n",
    "        return {\n",
    "            0: 1.0,\n",
    "            1: 1.0,\n",
    "            2: 1.0,\n",
    "            3: 1.0,\n",
    "            4: 1.0,\n",
    "            5: 1.0\n",
    "        }\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
