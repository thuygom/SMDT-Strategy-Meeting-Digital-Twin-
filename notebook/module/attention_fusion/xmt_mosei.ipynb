{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358a5b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from mmsdk import mmdatasdk\n",
    "\n",
    "# ==========================================\n",
    "# 1. GPU 설정\n",
    "# ==========================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[Info] Using device: {device}\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. Helper Functions\n",
    "# ==========================================\n",
    "def get_avg_feature(seq):\n",
    "    try:\n",
    "        if isinstance(seq, dict):\n",
    "            features = seq.get(\"features\", None)\n",
    "        else:\n",
    "            features = seq\n",
    "        if features is not None:\n",
    "            features = features[:]  # HDF5 → ndarray\n",
    "        avg = np.nanmean(features, axis=0)\n",
    "        return avg\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def safe_tensor(tensor_np):\n",
    "    # NaN만 0으로 대체, inf는 유지\n",
    "    tensor_np = np.nan_to_num(tensor_np, nan=0.0, posinf=np.inf, neginf=-np.inf)\n",
    "    return torch.tensor(tensor_np, dtype=torch.float32).to(device)\n",
    "\n",
    "def cross_modal_fusion(vectors):\n",
    "    \"\"\"\n",
    "    vectors: List[Tensor] → 각 모달의 임베딩 (torch.tensor shape: [d])\n",
    "    return: torch.tensor shape [d]\n",
    "    \"\"\"\n",
    "    valid_vectors = []\n",
    "    for vec in vectors:\n",
    "        if not torch.isinf(vec).any():\n",
    "            valid_vectors.append(vec)\n",
    "\n",
    "    # 모든 모달이 invalid이면 zero vector 반환\n",
    "    if not valid_vectors:\n",
    "        return torch.zeros_like(vectors[0]).cpu().numpy()\n",
    "\n",
    "    # Query-Key-Value 구조로 각각을 서로에게 적용\n",
    "    fused = torch.zeros_like(vectors[0])\n",
    "    d = vectors[0].shape[-1]\n",
    "\n",
    "    for i, q in enumerate(valid_vectors):\n",
    "        others = [v for j, v in enumerate(valid_vectors) if j != i]\n",
    "        if not others:\n",
    "            continue\n",
    "        keys = torch.stack(others)  # [n-1, d]\n",
    "        attn_scores = torch.matmul(q.unsqueeze(0), keys.T) / (d ** 0.5)\n",
    "        weights = torch.softmax(attn_scores, dim=-1)\n",
    "        value = torch.matmul(weights, keys)\n",
    "        fused += value.squeeze(0)\n",
    "\n",
    "    fused = fused / len(valid_vectors)  # 평균\n",
    "    return fused.detach().cpu().numpy()\n",
    "\n",
    "# ==========================================\n",
    "# 3. Load Dataset\n",
    "# ==========================================\n",
    "data_path = \"D:/CMU-MultimodalSDK/data/mosei\"\n",
    "features = {\n",
    "    \"glove_vectors\": os.path.join(data_path, \"CMU_MOSEI_TimestampedWordVectors.csd\"),\n",
    "    \"COVAREP\": os.path.join(data_path, \"CMU_MOSEI_COVAREP.csd\"),\n",
    "    \"FACET\": os.path.join(data_path, \"CMU_MOSEI_VisualFacet42.csd\"),\n",
    "    \"labels\": os.path.join(data_path, \"CMU_MOSEI_Labels.csd\"),\n",
    "}\n",
    "dataset = mmdatasdk.mmdataset(features, data_path)\n",
    "\n",
    "# ==========================================\n",
    "# 4. Get Common Segments\n",
    "# ==========================================\n",
    "def get_segment_ids(data):\n",
    "    return set((vid, seg_id) for vid in data for seg_id in data[vid])\n",
    "\n",
    "glove_ids = get_segment_ids(dataset[\"glove_vectors\"].data)\n",
    "covarep_ids = get_segment_ids(dataset[\"COVAREP\"].data)\n",
    "facet_ids = get_segment_ids(dataset[\"FACET\"].data)\n",
    "label_ids = get_segment_ids(dataset[\"labels\"].data)\n",
    "common_ids = glove_ids & covarep_ids & facet_ids & label_ids\n",
    "\n",
    "# ==========================================\n",
    "# 5. Define projection layers (GPU)\n",
    "# ==========================================\n",
    "linear_text = nn.Linear(300, 768).to(device)\n",
    "linear_audio = nn.Linear(74, 768).to(device)\n",
    "linear_visual = nn.Linear(35, 768).to(device)\n",
    "\n",
    "# ==========================================\n",
    "# 6. Build Dataset\n",
    "# ==========================================\n",
    "def build_dataset(dataset, common_ids):\n",
    "    X, y = [], []\n",
    "    total, dropped = 0, 0\n",
    "    drop_reason = {\"label_nan\": 0, \"interval\": 0, \"empty_modality\": 0, \"exception\": 0}\n",
    "\n",
    "    print(\"\\n[Info] Building dataset with Attention Fusion...\")\n",
    "    for vid, seg_id in tqdm(common_ids, desc=\"🔄 Processing\", unit=\"segment\"):\n",
    "        total += 1\n",
    "        try:\n",
    "            label = dataset[\"labels\"].data[vid][seg_id][:]\n",
    "            if label.shape[1] != 7:\n",
    "                dropped += 1\n",
    "                drop_reason[\"interval\"] += 1\n",
    "                continue\n",
    "\n",
    "            label = label.mean(axis=0)  # shape: (7,)\n",
    "\n",
    "            # NaN 또는 음수 클리어\n",
    "            label = np.nan_to_num(label, nan=0.0)\n",
    "            label = np.clip(label, a_min=0.0, a_max=None)\n",
    "\n",
    "            label_sum = label.sum()\n",
    "            if label_sum == 0:\n",
    "                dropped += 1\n",
    "                drop_reason[\"label_nan\"] += 1\n",
    "                continue\n",
    "            label = label / label_sum  # 정규화된 soft label\n",
    "\n",
    "            text_feat = get_avg_feature(dataset[\"glove_vectors\"].data[vid][seg_id])\n",
    "            audio_feat = get_avg_feature(dataset[\"COVAREP\"].data[vid][seg_id])\n",
    "            visual_feat = get_avg_feature(dataset[\"FACET\"].data[vid][seg_id])\n",
    "\n",
    "            if text_feat is None or audio_feat is None or visual_feat is None:\n",
    "                dropped += 1\n",
    "                drop_reason[\"empty_modality\"] += 1\n",
    "                continue\n",
    "\n",
    "            proj_text = linear_text(safe_tensor(text_feat))\n",
    "            proj_audio = linear_audio(safe_tensor(audio_feat))\n",
    "            proj_visual = linear_visual(safe_tensor(visual_feat))\n",
    "\n",
    "            fused = cross_modal_fusion([proj_text, proj_audio, proj_visual])\n",
    "\n",
    "            if np.isnan(fused).any():\n",
    "                dropped += 1\n",
    "                drop_reason[\"label_nan\"] += 1\n",
    "                continue\n",
    "\n",
    "            X.append(fused)\n",
    "            y.append(label)\n",
    "\n",
    "        except Exception as e:\n",
    "            dropped += 1\n",
    "            drop_reason[\"exception\"] += 1\n",
    "            print(f\"[Error] {vid} / {seg_id} → {type(e).__name__}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(\"\\n[Debug] Dataset Summary\")\n",
    "    print(f\" - 총 시도한 세그먼트 수: {total}\")\n",
    "    print(f\" - 누락된 세그먼트 수: {dropped}\")\n",
    "    print(f\"    • NaN 라벨로 제거됨: {drop_reason['label_nan']}\")\n",
    "    print(f\"    • 비어있는 modality 제거됨: {drop_reason['empty_modality']}\")\n",
    "    print(f\"    • 예외 처리로 제거됨: {drop_reason['interval']}\")\n",
    "    print(f\" - 최종 학습 데이터 수: {len(X)}\")\n",
    "\n",
    "    if X:\n",
    "        print(f\" - 입력 벡터 shape: {X[0].shape}\")\n",
    "        for i in range(min(5, len(X))):\n",
    "            print(f\"\\n🧪 샘플 {i+1}\")\n",
    "            print(f\"  • 소프트 라벨: {y[i]}\")\n",
    "            print(f\"  • 벡터 일반 (20): {X[i][:20]}\")\n",
    "    else:\n",
    "        print(\"❌ 유효한 데이터가 없어 학습 불가\")\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# ==========================================\n",
    "# 7. Run\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    X, y = build_dataset(dataset, common_ids)\n",
    "    np.save(\"x_xmt.npy\", X)\n",
    "    np.save(\"y_xmt.npy\", y)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
